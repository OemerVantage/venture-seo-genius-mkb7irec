name: Codex Build

on:
  workflow_dispatch:
    inputs:
      build_run_id:
        description: 'Build Run ID for callbacks'
        required: true
        type: string
      start_task:
        description: 'Start from task number'
        default: '1'
        type: string
      mode:
        description: 'Execution mode'
        default: 'auto'
        type: choice
        options:
          - auto
          - manual
          - dry-run

permissions:
  contents: write
  pull-requests: write

env:
  DOCS_PATH: ./docs
  SUPABASE_URL: https://gfqjhanjogtnnwnujafd.supabase.co
  SUPABASE_ANON_KEY: sb_publishable_8vW5D5wEAyKAW8y4nRPaLA_QEoYCEP6
  WEBHOOK_URL: https://gfqjhanjogtnnwnujafd.supabase.co/functions/v1/github-webhook
  BUILD_RUN_ID: ${{ github.event.inputs.build_run_id }}

jobs:
  notify-start:
    name: Notify Build Started
    runs-on: ubuntu-latest
    steps:
      - name: Send start notification
        run: |
          curl -X POST "${{ env.WEBHOOK_URL }}" \
            -H "Content-Type: application/json" \
            -d '{
              "event_type": "started",
              "build_run_id": "'${{ env.BUILD_RUN_ID }}'",
              "workflow_run_id": "'${{ github.run_id }}'",
              "timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'"
            }'

  read-mission:
    name: Read Mission & Tasks
    runs-on: ubuntu-latest
    needs: notify-start
    outputs:
      task_count: ${{ steps.count.outputs.count }}
      prd_exists: ${{ steps.check.outputs.prd }}
    steps:
      - uses: actions/checkout@v4
      
      - name: Display Mission
        run: |
          echo "üìã MISSION BRIEFING"
          echo "===================="
          cat docs/MISSION.md
          
      - name: Check docs exist
        id: check
        run: |
          [ -f docs/PRD.md ] && echo "prd=true" >> $GITHUB_OUTPUT || echo "prd=false" >> $GITHUB_OUTPUT
          
      - name: Count tasks
        id: count
        run: |
          TASK_COUNT=$(grep -c "^## Task" docs/TASKS.md || echo "5")
          echo "count=$TASK_COUNT" >> $GITHUB_OUTPUT
          echo "üìä Found $TASK_COUNT tasks"

  setup-project:
    name: Setup Project Structure
    runs-on: ubuntu-latest
    needs: read-mission
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          
      - name: Initialize project (if needed)
        run: |
          if [ ! -f "package.json" ]; then
            echo "üì¶ Initializing new project..."
            npm create vite@latest . -- --template react-ts --yes || true
            npm install -D tailwindcss postcss autoprefixer
            npm install @supabase/supabase-js @tanstack/react-query
          else
            echo "‚úÖ Project already initialized"
            npm install
          fi
          
      - name: Create .env.example
        run: |
          cat > .env.example << 'EOF'
          VITE_SUPABASE_URL=your-supabase-url
          VITE_SUPABASE_ANON_KEY=your-anon-key
          EOF
          
      - name: Commit setup changes
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "üõ†Ô∏è Project setup complete"
          branch: ${{ github.ref_name }}

  execute-tasks:
    name: Execute Task ${{ matrix.task }}
    runs-on: ubuntu-latest
    needs: [read-mission, setup-project]
    if: ${{ github.event.inputs.mode != 'dry-run' }}
    strategy:
      max-parallel: 1
      fail-fast: false
      matrix:
        task: [1, 2, 3, 4, 5]
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.ref_name }}
          fetch-depth: 0
          
      - name: Pull latest changes
        run: git pull origin ${{ github.ref_name }} || true

      - name: Check if task should run
        id: should_run
        run: |
          START=${{ github.event.inputs.start_task || '1' }}
          CURRENT=${{ matrix.task }}
          if [ "$CURRENT" -ge "$START" ]; then
            echo "run=true" >> $GITHUB_OUTPUT
          else
            echo "run=false" >> $GITHUB_OUTPUT
            echo "‚è≠Ô∏è Skipping task $CURRENT (starting from $START)"
          fi
          
      - name: Read Task Details
        if: steps.should_run.outputs.run == 'true'
        id: task
        run: |
          TASK_CONTENT=$(sed -n "/^## Task ${{ matrix.task }}:/,/^## Task/p" docs/TASKS.md | head -n -1)
          if [ -z "$TASK_CONTENT" ]; then
            # Try alternative format
            TASK_CONTENT=$(sed -n "/^## Task ${{ matrix.task }}[^0-9]/,/^## Task/p" docs/TASKS.md | head -n -1)
          fi
          if [ -z "$TASK_CONTENT" ]; then
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è Task ${{ matrix.task }} not found"
          else
            echo "exists=true" >> $GITHUB_OUTPUT
            echo "üìå Task ${{ matrix.task }} found"
            # Save to file for later use (handles multi-line content)
            echo "$TASK_CONTENT" > /tmp/current_task.md
          fi

      - name: Notify Task Started
        if: steps.should_run.outputs.run == 'true' && steps.task.outputs.exists == 'true'
        run: |
          curl -X POST "${{ env.WEBHOOK_URL }}" \
            -H "Content-Type: application/json" \
            -d '{
              "event_type": "task_started",
              "build_run_id": "'${{ env.BUILD_RUN_ID }}'",
              "task_number": ${{ matrix.task }},
              "total_tasks": 5,
              "timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'"
            }'

      - name: Read Context Files
        if: steps.should_run.outputs.run == 'true' && steps.task.outputs.exists == 'true'
        id: context
        run: |
          # Read PRD
          PRD=""
          if [ -f "docs/PRD.md" ]; then
            PRD=$(cat docs/PRD.md)
          fi
          
          # Read Tech Spec
          TECH_SPEC=""
          if [ -f "docs/TECH_SPEC.md" ]; then
            TECH_SPEC=$(cat docs/TECH_SPEC.md)
          fi
          
          # Read previous task summary if exists
          PREV_CONTEXT=""
          PREV_TASK=$((${{ matrix.task }} - 1))
          if [ -f "docs/task_${PREV_TASK}_summary.md" ]; then
            PREV_CONTEXT=$(cat docs/task_${PREV_TASK}_summary.md)
          fi
          
          # Save to files for multiline content
          echo "$PRD" > /tmp/prd.md
          echo "$TECH_SPEC" > /tmp/tech_spec.md
          echo "$PREV_CONTEXT" > /tmp/prev_context.md
          
      - name: Self-Healing Code Generation Loop
        if: steps.should_run.outputs.run == 'true' && steps.task.outputs.exists == 'true'
        id: ai_loop
        run: |
          MAX_ITERATIONS=3
          ITERATION=1
          APPROVED="false"
          FIXES_APPLIED_TOTAL=0
          
          # Read context files once
          PRD=$(cat /tmp/prd.md 2>/dev/null || echo "")
          TECH_SPEC=$(cat /tmp/tech_spec.md 2>/dev/null || echo "")
          TASK=$(cat /tmp/current_task.md)
          PREV_CONTEXT=$(cat /tmp/prev_context.md 2>/dev/null || echo "")
          
          while [ "$ITERATION" -le "$MAX_ITERATIONS" ] && [ "$APPROVED" = "false" ]; do
            echo "üîÑ === Iteration $ITERATION von $MAX_ITERATIONS ==="
            
            # Build review feedback for subsequent iterations
            REVIEW_FEEDBACK=""
            if [ "$ITERATION" -gt 1 ] && [ -f "/tmp/review_response.json" ]; then
              REVIEW_FEEDBACK=$(jq -r '.findings // [] | map("- [(.severity)] (.file):(.line // "?") - (.issue) ‚Üí (.suggestion // "Fix required")") | join("\n")' /tmp/review_response.json 2>/dev/null || echo "")
              echo "üìù Sending $(echo "$REVIEW_FEEDBACK" | wc -l) findings as feedback"
            fi
            
            # 1. Generate Code with AI Codex
            echo "ü§ñ Calling AI Codex (iteration $ITERATION)..."
            
            RESPONSE=$(curl -s -w "\n%{http_code}" -X POST "${{ env.SUPABASE_URL }}/functions/v1/ai-codex" \
              -H "Content-Type: application/json" \
              -H "Authorization: Bearer ${{ env.SUPABASE_ANON_KEY }}" \
              -d @- << EOF
            {
              "task_number": ${{ matrix.task }},
              "prd": $(echo "$PRD" | jq -Rs .),
              "tech_spec": $(echo "$TECH_SPEC" | jq -Rs .),
              "task": $(echo "$TASK" | jq -Rs .),
              "previous_context": $(echo "$PREV_CONTEXT" | jq -Rs .),
              "review_feedback": $(echo "$REVIEW_FEEDBACK" | jq -Rs .),
              "iteration": $ITERATION,
              "build_run_id": "${{ env.BUILD_RUN_ID }}"
            }
            EOF
            )
            
            HTTP_CODE=$(echo "$RESPONSE" | tail -n1)
            BODY=$(echo "$RESPONSE" | sed '$d')
            
            if [ "$HTTP_CODE" -ge 400 ]; then
              echo "‚ùå AI Codex error: HTTP $HTTP_CODE"
              echo "$BODY"
              if [ "$ITERATION" -lt "$MAX_ITERATIONS" ]; then
                echo "‚è≥ Retrying in 10 seconds..."
                sleep 10
                ITERATION=$((ITERATION + 1))
                continue
              else
                exit 1
              fi
            fi
            
            echo "$BODY" > /tmp/ai_response.json
            SUMMARY=$(echo "$BODY" | jq -r '.summary // "No summary"')
            echo "üìù Summary: $SUMMARY"
            
            # 2. Write Generated Files
            echo "üìÅ Writing generated files..."
            FILES_COUNT=$(jq '.files | length' /tmp/ai_response.json)
            echo "Found $FILES_COUNT files to write"
            
            for i in $(seq 0 $(($FILES_COUNT - 1))); do
              FILE_PATH=$(jq -r ".files[$i].path" /tmp/ai_response.json)
              FILE_ACTION=$(jq -r ".files[$i].action // \"create\"" /tmp/ai_response.json)
              
              if [ "$FILE_ACTION" = "delete" ]; then
                echo "üóëÔ∏è Deleting: $FILE_PATH"
                rm -f "$FILE_PATH"
              else
                echo "üìù Writing: $FILE_PATH"
                mkdir -p "$(dirname "$FILE_PATH")"
                jq -r ".files[$i].content" /tmp/ai_response.json > "$FILE_PATH"
              fi
            done
            
            # 3. Review Generated Code
            echo "üîç Starting AI Code Review..."
            FILES_JSON=$(jq -c '.files' /tmp/ai_response.json)
            
            REVIEW_RESPONSE=$(curl -s -w "\n%{http_code}" -X POST "${{ env.SUPABASE_URL }}/functions/v1/ai-review" \
              -H "Content-Type: application/json" \
              -H "Authorization: Bearer ${{ env.SUPABASE_ANON_KEY }}" \
              -d @- << EOF
            {
              "files": $FILES_JSON,
              "task_number": ${{ matrix.task }},
              "prd": $(echo "$PRD" | jq -Rs .),
              "tech_spec": $(echo "$TECH_SPEC" | jq -Rs .),
              "build_run_id": "${{ env.BUILD_RUN_ID }}"
            }
            EOF
            )
            
            REVIEW_HTTP_CODE=$(echo "$REVIEW_RESPONSE" | tail -n1)
            REVIEW_BODY=$(echo "$REVIEW_RESPONSE" | sed '$d')
            
            if [ "$REVIEW_HTTP_CODE" -ge 400 ]; then
              echo "‚ö†Ô∏è Review API error: HTTP $REVIEW_HTTP_CODE (skipping review)"
              echo '{"approved": true, "findings": [], "summary": "Review skipped", "fixes": [], "critical_count": 0}' > /tmp/review_response.json
            else
              echo "$REVIEW_BODY" > /tmp/review_response.json
            fi
            
            # Check review result
            APPROVED=$(jq -r '.approved // true' /tmp/review_response.json)
            CRITICAL_COUNT=$(jq -r '.critical_count // 0' /tmp/review_response.json)
            WARNING_COUNT=$(jq -r '.warning_count // 0' /tmp/review_response.json)
            REVIEW_SUMMARY=$(jq -r '.summary // "Review passed"' /tmp/review_response.json)
            
            echo "üìã Code Review Results:"
            echo "   Approved: $APPROVED"
            echo "   Critical: $CRITICAL_COUNT"
            echo "   Warnings: $WARNING_COUNT"
            echo "   Summary: $REVIEW_SUMMARY"
            
            # 4. Apply fixes if provided and not approved
            if [ "$APPROVED" = "false" ]; then
              FIXES_COUNT=$(jq '.fixes | length' /tmp/review_response.json)
              if [ "$FIXES_COUNT" -gt 0 ]; then
                echo "üîß Applying $FIXES_COUNT fixes from review..."
                for i in $(seq 0 $(($FIXES_COUNT - 1))); do
                  FILE_PATH=$(jq -r ".fixes[$i].path" /tmp/review_response.json)
                  echo "   Fixing: $FILE_PATH"
                  mkdir -p "$(dirname "$FILE_PATH")"
                  jq -r ".fixes[$i].content" /tmp/review_response.json > "$FILE_PATH"
                done
                FIXES_APPLIED_TOTAL=$((FIXES_APPLIED_TOTAL + FIXES_COUNT))
              fi
              
              # Check if we should retry
              if [ "$CRITICAL_COUNT" -gt 0 ] && [ "$ITERATION" -lt "$MAX_ITERATIONS" ]; then
                echo "‚ö†Ô∏è $CRITICAL_COUNT critical issues remain - will retry with feedback"
                ITERATION=$((ITERATION + 1))
              else
                echo "‚úÖ No critical issues or max iterations reached - proceeding"
                APPROVED="true"
              fi
            else
              echo "‚úÖ Code approved on iteration $ITERATION"
            fi
          done
          
          # Save final state
          echo "iterations=$ITERATION" >> $GITHUB_OUTPUT
          echo "approved=$APPROVED" >> $GITHUB_OUTPUT
          echo "critical_count=$(jq -r '.critical_count // 0' /tmp/review_response.json)" >> $GITHUB_OUTPUT
          echo "warning_count=$(jq -r '.warning_count // 0' /tmp/review_response.json)" >> $GITHUB_OUTPUT
          echo "fixes_applied=$FIXES_APPLIED_TOTAL" >> $GITHUB_OUTPUT
          
          echo ""
          echo "üèÅ Self-Healing Loop completed after $ITERATION iteration(s)"
          echo "   Total fixes applied: $FIXES_APPLIED_TOTAL"

      - name: Install Dependencies (if needed)
        if: steps.should_run.outputs.run == 'true' && steps.task.outputs.exists == 'true'
        run: |
          NEW_DEPS=$(jq -r '.dependencies // [] | join(" ")' /tmp/ai_response.json)
          if [ -n "$NEW_DEPS" ] && [ "$NEW_DEPS" != "" ]; then
            echo "üì¶ Installing new dependencies: $NEW_DEPS"
            npm install $NEW_DEPS
          fi

      - name: Generate Unit Tests with AI
        if: steps.should_run.outputs.run == 'true' && steps.task.outputs.exists == 'true'
        id: ai_tests
        run: |
          echo "üß™ Generating unit tests..."
          
          # Read generated files for test generation
          FILES_JSON=$(jq -c '.files' /tmp/ai_response.json)
          
          # Call AI Test Generator
          TEST_RESPONSE=$(curl -s -w "\n%{http_code}" -X POST "${{ env.SUPABASE_URL }}/functions/v1/ai-test-gen" \
            -H "Content-Type: application/json" \
            -H "Authorization: Bearer ${{ env.SUPABASE_ANON_KEY }}" \
            -d @- << EOF
          {
            "files": $FILES_JSON,
            "task_number": ${{ matrix.task }},
            "build_run_id": "${{ env.BUILD_RUN_ID }}"
          }
          EOF
          )
          
          HTTP_CODE=$(echo "$TEST_RESPONSE" | tail -n1)
          BODY=$(echo "$TEST_RESPONSE" | sed '$d')
          
          if [ "$HTTP_CODE" -ge 400 ]; then
            echo "‚ö†Ô∏è Test generation API error: HTTP $HTTP_CODE (skipping tests)"
            echo '{"test_files": [], "setup_required": false, "dependencies": [], "summary": "Test generation skipped"}' > /tmp/test_response.json
            echo "test_count=0" >> $GITHUB_OUTPUT
          else
            echo "$BODY" > /tmp/test_response.json
            
            # Count generated tests
            TEST_COUNT=$(jq '.test_files | length' /tmp/test_response.json)
            SUMMARY=$(jq -r '.summary // "Tests generated"' /tmp/test_response.json)
            echo "test_count=$TEST_COUNT" >> $GITHUB_OUTPUT
            echo "üìù $SUMMARY"
            echo "   Generated $TEST_COUNT test file(s)"
          fi

      - name: Write Test Files
        if: steps.should_run.outputs.run == 'true' && steps.task.outputs.exists == 'true' && steps.ai_tests.outputs.test_count > 0
        run: |
          echo "üìù Writing test files..."
          
          TEST_COUNT=$(jq '.test_files | length' /tmp/test_response.json)
          
          for i in $(seq 0 $(($TEST_COUNT - 1))); do
            FILE_PATH=$(jq -r ".test_files[$i].path" /tmp/test_response.json)
            echo "   Writing: $FILE_PATH"
            mkdir -p "$(dirname "$FILE_PATH")"
            jq -r ".test_files[$i].content" /tmp/test_response.json > "$FILE_PATH"
          done

      - name: Setup Vitest (if needed)
        if: steps.should_run.outputs.run == 'true' && steps.task.outputs.exists == 'true' && steps.ai_tests.outputs.test_count > 0
        run: |
          # Install test dependencies
          SETUP_REQUIRED=$(jq -r '.setup_required // false' /tmp/test_response.json)
          
          if [ "$SETUP_REQUIRED" = "true" ]; then
            TEST_DEPS=$(jq -r '.dependencies // [] | join(" ")' /tmp/test_response.json)
            if [ -n "$TEST_DEPS" ] && [ "$TEST_DEPS" != "" ]; then
              echo "üì¶ Installing test dependencies: $TEST_DEPS"
              npm install -D $TEST_DEPS
            fi
          fi
          
          # Create vitest config if not exists
          if [ ! -f "vitest.config.ts" ]; then
            echo "üìÑ Creating vitest.config.ts..."
            cat > vitest.config.ts << 'VITEST_EOF'
          import { defineConfig } from 'vitest/config'
          import react from '@vitejs/plugin-react'
          import path from 'path'

          export default defineConfig({
            plugins: [react()],
            test: {
              environment: 'jsdom',
              globals: true,
              setupFiles: './src/test/setup.ts',
            },
            resolve: {
              alias: {
                '@': path.resolve(__dirname, './src'),
              },
            },
          })
          VITEST_EOF
            
            # Create test setup file
            mkdir -p src/test
            cat > src/test/setup.ts << 'SETUP_EOF'
          import '@testing-library/jest-dom'
          SETUP_EOF
          fi

      - name: Run Tests
        if: steps.should_run.outputs.run == 'true' && steps.task.outputs.exists == 'true' && steps.ai_tests.outputs.test_count > 0
        id: run_tests
        continue-on-error: true
        run: |
          echo "üß™ Running tests..."
          
          # Add test script to package.json if not exists
          if ! jq -e '.scripts.test' package.json > /dev/null 2>&1; then
            jq '.scripts.test = "vitest run"' package.json > package.json.tmp && mv package.json.tmp package.json
          fi
          
          # Run tests and capture output
          npm run test -- --run 2>&1 | tee /tmp/test_output.txt || true
          
          TEST_EXIT_CODE=${PIPESTATUS[0]}
          
          # Parse test results
          PASSED=$(grep -c "‚úì" /tmp/test_output.txt 2>/dev/null || echo "0")
          FAILED=$(grep -c "‚úó\|√ó" /tmp/test_output.txt 2>/dev/null || echo "0")
          
          echo "passed=$PASSED" >> $GITHUB_OUTPUT
          echo "failed=$FAILED" >> $GITHUB_OUTPUT
          echo "exit_code=$TEST_EXIT_CODE" >> $GITHUB_OUTPUT
          
          echo "üìä Test Results: $PASSED passed, $FAILED failed"

      - name: Save Review Report
        if: steps.should_run.outputs.run == 'true' && steps.task.outputs.exists == 'true'
        run: |
          mkdir -p docs/reviews
          REPORT_FILE="docs/reviews/task_${{ matrix.task }}_review.md"
          
          APPROVED=$(jq -r '.approved // true' /tmp/review_response.json)
          SUMMARY=$(jq -r '.summary // "No summary"' /tmp/review_response.json)
          CRITICAL=$(jq -r '.critical_count // 0' /tmp/review_response.json)
          WARNINGS=$(jq -r '.warning_count // 0' /tmp/review_response.json)
          ITERATIONS=${{ steps.ai_loop.outputs.iterations }}
          FIXES=${{ steps.ai_loop.outputs.fixes_applied }}
          
          cat > "$REPORT_FILE" << EOF
          # Code Review - Task ${{ matrix.task }}
          
          **Status:** $([ "$APPROVED" = "true" ] && echo "‚úÖ Approved" || echo "‚ö†Ô∏è Fixes Required")
          **Date:** $(date -u +%Y-%m-%dT%H:%M:%SZ)
          **Iterations:** $ITERATIONS / 3
          **Critical Issues:** $CRITICAL
          **Warnings:** $WARNINGS
          **Fixes Applied:** $FIXES
          
          ## Summary
          $SUMMARY
          
          ## Findings
          EOF
          
          jq -r '.findings[] | "- **[(.severity)]** (.file):(.line // "?") - (.issue)"' /tmp/review_response.json >> "$REPORT_FILE" 2>/dev/null || echo "No findings" >> "$REPORT_FILE"
          
          echo "" >> "$REPORT_FILE"
          echo "---" >> "$REPORT_FILE"
          echo "*Generated by AI Code Review with Self-Healing*" >> "$REPORT_FILE"

      - name: Save Test Report
        if: steps.should_run.outputs.run == 'true' && steps.task.outputs.exists == 'true' && steps.ai_tests.outputs.test_count > 0
        run: |
          mkdir -p docs/tests
          REPORT_FILE="docs/tests/task_${{ matrix.task }}_tests.md"
          
          TEST_COUNT=${{ steps.ai_tests.outputs.test_count }}
          PASSED=${{ steps.run_tests.outputs.passed || 0 }}
          FAILED=${{ steps.run_tests.outputs.failed || 0 }}
          
          cat > "$REPORT_FILE" << EOF
          # Test Report - Task ${{ matrix.task }}
          
          **Generated:** $(date -u +%Y-%m-%dT%H:%M:%SZ)
          **Test Files:** $TEST_COUNT
          **Passed:** $PASSED
          **Failed:** $FAILED
          
          ## Test Output
          \`\`\`
          $(cat /tmp/test_output.txt 2>/dev/null | head -100 || echo "No test output")
          \`\`\`
          
          ---
          *Generated by AI Test Generator*
          EOF

      - name: Update Progress
        if: steps.should_run.outputs.run == 'true' && steps.task.outputs.exists == 'true'
        run: |
          SUMMARY=$(jq -r '.summary // "Task completed"' /tmp/ai_response.json)
          ITERATIONS=${{ steps.ai_loop.outputs.iterations }}
          TEST_COUNT=${{ steps.ai_tests.outputs.test_count || 0 }}
          
          echo "" >> docs/PROGRESS.md
          echo "## Task ${{ matrix.task }}" >> docs/PROGRESS.md
          echo "**Status:** ‚úÖ Completed" >> docs/PROGRESS.md
          echo "**Iterations:** $ITERATIONS" >> docs/PROGRESS.md
          echo "**Tests Generated:** $TEST_COUNT" >> docs/PROGRESS.md
          echo "**Completed:** $(date -u +%Y-%m-%dT%H:%M:%SZ)" >> docs/PROGRESS.md
          echo "**Summary:** $SUMMARY" >> docs/PROGRESS.md

      - name: Commit Generated Code + Tests
        if: steps.should_run.outputs.run == 'true' && steps.task.outputs.exists == 'true'
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "‚ú® Task ${{ matrix.task }} - AI implementation (iterations: ${{ steps.ai_loop.outputs.iterations }}, tests: ${{ steps.ai_tests.outputs.test_count || 0 }})"
          branch: ${{ github.ref_name }}

      - name: Notify Task Completed
        if: steps.should_run.outputs.run == 'true' && steps.task.outputs.exists == 'true'
        run: |
          SUMMARY=$(jq -r '.summary // "Task completed"' /tmp/ai_response.json 2>/dev/null || echo "Task ${{ matrix.task }} completed")
          REVIEW_APPROVED=$(jq -r '.approved // true' /tmp/review_response.json 2>/dev/null || echo "true")
          CRITICAL_COUNT=$(jq -r '.critical_count // 0' /tmp/review_response.json 2>/dev/null || echo "0")
          REVIEW_FINDINGS=$(jq -r '[.findings[] | select(.severity != "info")] | length' /tmp/review_response.json 2>/dev/null || echo "0")
          FIXES_APPLIED=${{ steps.ai_loop.outputs.fixes_applied || 0 }}
          ITERATIONS=${{ steps.ai_loop.outputs.iterations || 1 }}
          TEST_COUNT=${{ steps.ai_tests.outputs.test_count || 0 }}
          TESTS_PASSED=${{ steps.run_tests.outputs.passed || 0 }}
          TESTS_FAILED=${{ steps.run_tests.outputs.failed || 0 }}
          
          curl -X POST "${{ env.WEBHOOK_URL }}" \
            -H "Content-Type: application/json" \
            -d '{
              "event_type": "task_completed",
              "build_run_id": "'${{ env.BUILD_RUN_ID }}'",
              "task_number": ${{ matrix.task }},
              "total_tasks": 5,
              "message": "'$SUMMARY'",
              "review_approved": '$REVIEW_APPROVED',
              "review_findings": '$REVIEW_FINDINGS',
              "fixes_applied": '$FIXES_APPLIED',
              "iterations": '$ITERATIONS',
              "tests_generated": '$TEST_COUNT',
              "tests_passed": '$TESTS_PASSED',
              "tests_failed": '$TESTS_FAILED',
              "timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'"
            }'

      - name: Notify Error
        if: failure()
        run: |
          ITERATIONS=${{ steps.ai_loop.outputs.iterations || 1 }}
          curl -X POST "${{ env.WEBHOOK_URL }}" \
            -H "Content-Type: application/json" \
            -d '{
              "event_type": "error",
              "build_run_id": "'${{ env.BUILD_RUN_ID }}'",
              "task_number": ${{ matrix.task }},
              "message": "Task ${{ matrix.task }} failed after '$ITERATIONS' iteration(s)",
              "iterations": '$ITERATIONS',
              "timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'"
            }'

  create-pr:
    name: Create Pull Request
    runs-on: ubuntu-latest
    needs: execute-tasks
    if: always() && github.event.inputs.mode != 'dry-run'
    outputs:
      pr_url: ${{ steps.pr.outputs.pull-request-url }}
    steps:
      - uses: actions/checkout@v4
        with:
          ref: ${{ github.ref_name }}
          fetch-depth: 0
          
      - name: Pull latest changes
        run: git pull origin ${{ github.ref_name }} || true
          
      - name: Create PR
        id: pr
        uses: peter-evans/create-pull-request@v5
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          branch: ${{ github.ref_name }}
          base: main
          title: "üöÄ MVP Implementation from Venture OS"
          body: |
            ## Automated Build from Venture OS
            
            This PR was created by the AI Codex Build workflow.
            
            **Build Run ID:** ${{ github.event.inputs.build_run_id }}
            
            ### Documentation
            - üìã [PRD](./docs/PRD.md)
            - üîß [Tech Spec](./docs/TECH_SPEC.md)
            - ‚úÖ [Tasks](./docs/TASKS.md)
            - üìä [Progress](./docs/PROGRESS.md)
            
            ### AI Generated Summary
            See individual task summaries in `docs/task_*_summary.md`
            
            ### Review Checklist
            - [ ] All tasks completed
            - [ ] Code quality acceptable
            - [ ] No secrets committed
            - [ ] Tests pass (if any)
            
      - name: Notify PR Created
        if: steps.pr.outputs.pull-request-url
        run: |
          curl -X POST "${{ env.WEBHOOK_URL }}" \
            -H "Content-Type: application/json" \
            -d '{
              "event_type": "pr_created",
              "build_run_id": "'${{ env.BUILD_RUN_ID }}'",
              "pr_url": "'${{ steps.pr.outputs.pull-request-url }}'",
              "timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'"
            }'

  notify-completion:
    name: Notify Completion
    runs-on: ubuntu-latest
    needs: [create-pr]
    if: always()
    steps:
      - name: Send completion notification
        run: |
          STATUS="completed"
          if [ "${{ needs.create-pr.result }}" = "failure" ]; then
            STATUS="failed"
          fi
          
          curl -X POST "${{ env.WEBHOOK_URL }}" \
            -H "Content-Type: application/json" \
            -d '{
              "event_type": "completed",
              "build_run_id": "'${{ env.BUILD_RUN_ID }}'",
              "status": "'$STATUS'",
              "pr_url": "'${{ needs.create-pr.outputs.pr_url }}'",
              "timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'"
            }'

      - name: Summary
        run: |
          echo "## üéâ Build Complete!" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Build Run ID: ${{ github.event.inputs.build_run_id }}" >> $GITHUB_STEP_SUMMARY
          echo "Workflow finished with status: ${{ needs.create-pr.result }}" >> $GITHUB_STEP_SUMMARY
